{"cells":[{"cell_type":"code","source":["# --------------------------------------------------------\n#\n# PYTHON PROGRAM DEFINITION\n#\n# The knowledge a computer has of Python can be specified in 3 levels:\n# (1) Prelude knowledge --> The computer has it by default.\n# (2) Borrowed knowledge --> The computer gets this knowledge from 3rd party libraries defined by others\n#                            (but imported by us in this program).\n# (3) Generated knowledge --> The computer gets this knowledge from the new functions defined by us in this program.\n#\n# When launching in a terminal the command:\n# user:~$ python3 this_file.py\n# our computer first processes this PYTHON PROGRAM DEFINITION section of the file.\n# On it, our computer enhances its Python knowledge from levels (2) and (3) with the imports and new functions\n# defined in the program. However, it still does not execute anything.\n#\n# --------------------------------------------------------\n\n# ------------------------------------------\n# IMPORTS\n# ------------------------------------------\nimport pyspark\nimport shutil\nimport os\n\n# ------------------------------------------\n# FUNCTION my_main\n# ------------------------------------------\ndef my_main(sc, my_dataset_dir, longitude, num_lines):\n    # 1. Operation C1: Creation 'textFile', so as to store the content of the dataset into an RDD.\n\n    # The dataset was previously loaded into DBFS by:\n    # i) Place the desired dataset into your local file system folder %Test Environment%/my_dataset/\n    # ii) Running the script \"1.upload_dataset.bat\"\n\n    # Please note that, once again, the name textFile is a false friend here, as it seems that the parameter we are passing is the name of the file\n    # we want to read. Indeed, the parameter here is the name of a folder, and the dataset consists on the content of the files of the folder.\n    # An RDD of Strings is then generated, with one item per line of text in the files.\n    # In this case, the RDD is distributed among different machines (nodes) of our cluster.\n\n    #                            C1: textFile\n    # dataset: DBFS inputFolder -------------> inputRDD\n\n    inputRDD = sc.textFile(my_dataset_dir)\n\n    # 2. Operation T1: Transformation 'filter', so as to get a new RDD ('filteredRDD') with lines having at list length 'longitude'.\n\n    #         C1: textFile\n    # dataset -------------> inputRDD      --- RDD items are String ---\n    #                        |\n    #                        | T1: filter\n    #                        |------------> filteredRDD     --- RDD items are String ---\n\n    filteredRDD = inputRDD.filter(lambda x: len(x) >= longitude)\n\n    # 3. Operation A1: Action 'take', so as to take only a subset of the items of the RDD.\n\n    # It returns n elements from the RDD and attempts to minimize the number of partitions it accesses, so it may represent a biased collection.\n    # It's important to note that these operations do not return the elements in the order you might expect.\n\n\n    #         C1: textFile\n    # dataset -------------> inputRDD      --- RDD items are String ---\n    #                        |\n    #                        | T1: filter\n    #                        |------------> filteredRDD     --- RDD items are String ---\n    #                                       |\n    #                                       | A1: take\n    #                                       | ---------> resVAL     --- Iterator of items String ---\n\n    # This example is fantastic to explain what lazy evaluation is about.\n    # In this example, both inputRDD and filteredRDD are not computed when 'declared' in lines 32 and 42, resp.\n    # The Spark driver evaluates this program and says:\n    # Uhm, I clearly see 4 pieces of information here: [dataset, inputRDD, fiteredRDD, resVAL]\n    #                                                    DATA       RDD        RDD      VALUE\n    # Interestingly, when inputRDD was declared, nobody was requiring to check its internal state, so nothing was computed.\n    # Same story for filteredRDD, when it was declared nobody was requiring to check its internal state, so nothing was computed.\n\n    # Now, when running the Action A1: take(2), it is requesting 2 lines from filteredRDD to compute resVAL,\n    # so I'm sorry but filteredRDD has to be computed now.\n    # That said, does resVAL requires filteredRDD to be fully computed? Not at all.\n    # So we can think of resVAL and filteredRDD having A1: take(2) as their protocol of communication.\n    # In other words, two adults having a conversation to try to negotiate and reach a common agreement.\n    #  The communication from resVAL and filteredRDD (via the protocol A1: take(2)) can be as follows:\n    # \"filteredRDD, please give me two lines, this is more than enough for me\" - says resVAL.\n    # \"Ok, perfect\" - replies filteredRDD.\n    # As you see, the agreement here was easy :)\n\n    # Now let's move backwards to filteredRDD.\n    # It has to communicate with inputRDD. In this case, their protocol of communication is filter.\n    # So, once again, here we are in front of 2 adults having a conversation to try to negotiate and reach a common agreement.\n    #  The communication from filteredRDD and inputRDD (via the protocol T1: filter) can be as follows:\n\n    # \"inputRDD, according to filter, I will need you to be fully computed\" - filteredRDD will say.\n    # \"Oh no, really?\" - replies the lazy inputRDD -.\n    # \"Uhm, no, I also have some good news from resVAL, apparently he only needs 2 elements from me,\n    # so that means I only need 2 elements from you as well\" - filteredRDD continues -.\n    # \"Oh, this is great news! So, does this mean I only need to compute 2 elements myself\" - replies inputRDD, happy as a kid\n    # on her/his birthday.\n\n    # \"No, unfortunately I cannot guarantee you that\" -filteredRDD might reply-, \"maybe 2 is enough, but maybe you will need\n    # to compute 3 items, maybe 4, 5, ... or 1 million. Indeed, you will need to compute as many as needed, until 2\n    # of these items satisfies our protocol of communication filter\".\n    # \"Oh, I see\" -says inputRDD- \"What a pity!\".\n\n    # \"I know, but let's do something. Let's prceed as follows. You compute yourself one line at a time.\n    # Right after getting a new line you stop, and you come back to me, and we both ask our protocol filter if this line (item)\n    # passes the test or not. If it does, I check if this is the last line I really wanted, and if so, we're done.\n    # And, until this happens, we keep repeating this loop over and over again with one new line (item) at a time.\n    # Does it sound ok?\"\n\n    # \"Yes, it sound much better than computing myself entirely\" - inputRDD replies-.\n    # \"I know, lazy evaluation is great! Do you think all this story will make SDH4 students to understand lazy evaluation?\n    # or they might not even bother in reading this?\" -filteredRDD wonders-.\n    # \"I don't know, it's a great story, so I give a 50%-50% chance. But, anyway, why do we care? We are only RDDs, so this\n    # is not our business\" -inputRDD finishes-.\n\n    # So let's end the story and move backwards to inputRDD.\n    # It has to communicate with dataset. In this case, their protocol of communication is textFile.\n    # So, once again, here we are in front of 2 adults having a conversation to try to negotiate and reach a common agreement.\n    #  The communication from inputRDD and dataset (via the protocol C1: textFile) can be as follows:\n\n    # \"dataset, according to textFile, I will need you to pass me all your lines\" - inputRDD will say.\n    # \"Oh no, really?\" - replies the lazy dataset -.\n    # \"Uhm, no, I also have some kind of agreement with filteredRDD, apparently he only needs me to compute the lines 1 by 1\n    # until it is satisfied.\n    # So let's take advantage of this and have our own way to proceed here:\n    # I will ask you 1 line at a time. I might come back to you multiple times, requesting a new line, until either filteredRDD\n    # gets its 2 lines, or you give me the entire dataset, whatever happens first.\n    # Once I don't need more lines from you, I will let you know. Does it sound ok?\"\n\n    # \"Yes, it sound much better than doing the effort of passing you the entire dataset in one go\" - dataset replies-.\n    # \"I know, lazy evaluation is great! Do you think...\"\n    # \"Oh, please, stop, don't go again with this story again. Perfect, we have reach an agreement, so our job is done.\"\n    # -dataset finishes-.\n\n    # And this is how lazy evaluation works!\n\n    resVAL = filteredRDD.take(num_lines)\n\n    # 4. We print by the screen the collection computed in resVAL\n    for item in resVAL:\n        print(item)\n\n\n# --------------------------------------------------------\n#\n# PYTHON PROGRAM EXECUTION\n#\n# Once our computer has finished processing the PYTHON PROGRAM DEFINITION section its knowledge is set.\n# Now its time to apply this knowledge.\n#\n# When launching in a terminal the command:\n# user:~$ python3 this_file.py\n# our computer finally processes this PYTHON PROGRAM EXECUTION section, which:\n# (i) Specifies the function F to be executed.\n# (ii) Define any input parameter such this function F has to be called with.\n#\n# --------------------------------------------------------\nif __name__ == '__main__':\n    # 1. We use as many input arguments as needed\n    longitude = 20\n    num_lines = 2\n\n    # 2. Local or Databricks\n    local_False_databricks_True = False\n\n    # 3. We set the path to my_dataset and my_result\n    my_local_path = \"/\"\n    my_databricks_path = \"/\"\n\n    my_dataset_dir = \"FileStore/tables/1_Spark_Core/my_dataset/\"\n\n    if local_False_databricks_True == False:\n        my_dataset_dir = my_local_path + my_dataset_dir\n    else:\n        my_dataset_dir = my_databricks_path + my_dataset_dir\n\n    # 4. We configure the Spark Context\n    sc = pyspark.SparkContext.getOrCreate()\n    sc.setLogLevel('WARN')\n    print(\"\\n\\n\\n\")\n\n    # 5. We call to our main function\n    my_main(sc, my_dataset_dir, longitude, num_lines)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"378a7e23-64d0-4548-9491-118802384fb6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\n\n\n\n\tALL&#39;S WELL THAT ENDS WELL\nKING OF FRANCE\t(KING:)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\n\n\n\n\tALL&#39;S WELL THAT ENDS WELL\nKING OF FRANCE\t(KING:)\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"p09_take_(include_lazy_evaluation)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1468212715962400}},"nbformat":4,"nbformat_minor":0}
