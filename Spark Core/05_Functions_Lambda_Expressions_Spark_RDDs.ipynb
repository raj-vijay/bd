{"cells":[{"cell_type":"code","source":["# --------------------------------------------------------\n#\n# PYTHON PROGRAM DEFINITION\n#\n# The knowledge a computer has of Python can be specified in 3 levels:\n# (1) Prelude knowledge --> The computer has it by default.\n# (2) Borrowed knowledge --> The computer gets this knowledge from 3rd party libraries defined by others\n#                            (but imported by us in this program).\n# (3) Generated knowledge --> The computer gets this knowledge from the new functions defined by us in this program.\n#\n# When launching in a terminal the command:\n# user:~$ python3 this_file.py\n# our computer first processes this PYTHON PROGRAM DEFINITION section of the file.\n# On it, our computer enhances its Python knowledge from levels (2) and (3) with the imports and new functions\n# defined in the program. However, it still does not execute anything.\n#\n# --------------------------------------------------------\n\n# ------------------------------------------\n# IMPORTS\n# ------------------------------------------\nimport pyspark\n\n# ------------------------------------------\n# FUNCTION map_lambda\n# ------------------------------------------\ndef map_lambda(sc):\n    # 1. Operation C1: Creation 'parallelize', so as to store the content of the collection [1,2,3,4] into an RDD.\n\n    #         C1: parallelize\n    # dataset -----------------> inputRDD\n\n    inputRDD = sc.parallelize([1, 2, 3, 4, 5])\n\n    # 2. Operation T1: Transformation 'map', so as to get a new RDD ('squareRDD') from inputRDD.\n    # An RDD is inmutable, so it cannot be changed. However, you can apply a Transformation operation to get a new RDD2\n    # by applying some operation on the content of RDD1.\n\n    # The transformation operation 'map' is a higher order function.\n    # It requires as input arguments: (i) A function F and (ii) a collection of items C.\n    # It produces as output argument a new collection C' by applying F to each element of C.\n    # Example: map (+1) [1,2,3] = [2,3,4]\n    # where F is (+1), C is [1,2,3] and C' is [2,3,4]\n\n    # In our case, the collection C is always going to be RDD1, and C' the new RDD2.\n    # Thus, the only thing we are missing is specifying F.\n    # As you see, F must be a function receiving just 1 parameter (the item of C we want to apply F(C) to).\n\n    # In Spark we can define F via a lambda expression.\n\n    #         C1: parallelize             T1: map\n    # dataset -----------------> inputRDD --------> squareRDD\n\n    squareRDD = inputRDD.map(lambda x: x * x)\n\n    # 3. Operation A1: 'collect'.\n\n    #         C1: parallelize             T1: map             A1: collect\n    # dataset -----------------> inputRDD --------> squareRDD ------------> resVAL\n\n    resVAL = squareRDD.collect()\n\n    # 4. We print by the screen the collection computed in resVAL\n    for item in resVAL:\n        print(item)\n\n\n# ------------------------------------------\n# FUNCTION my_square_function\n# ------------------------------------------\ndef my_square_function(x):\n    # 1. We create the output variable\n    res = x * x\n\n    # 2. We return res\n    return res\n\n\n# ------------------------------------------\n# FUNCTION map_explicit_function\n# ------------------------------------------\ndef map_explicit_function(sc):\n    # 1. Operation C1: Creation 'parallelize', so as to store the content of the collection [1,2,3,4] into an RDD.\n\n    #         C1: parallelize\n    # dataset -----------------> inputRDD\n\n    inputRDD = sc.parallelize([1, 2, 3, 4, 5])\n\n    # 2. Operation T1: Transformation 'map', so as to get a new RDD ('squareRDD') from inputRDD.\n\n    # map requires as input arguments: (i) A function F and (ii) a collection of items C.\n    # map produces as output argument a new collection C' by applying F to each element of C.\n\n    # In our case, the collection C is always going to be RDD1, and C' the new RDD2.\n    # Thus, the only thing we are missing is specifying F.\n    # As you see, F must be a function receiving just 1 parameter (the item of C we want to apply F(C) to).\n\n    # In Spark we can define F via an explicit function. In this case we use the function my_square_function defined above.\n    # You might seem confused by the way the line of code below is actually written\n    # RDD2 = RDD1.map(F)\n    # but this is the way it works. Spark will apply F(c) for each c in RDD1\n\n    #         C1: parallelize             T1: map\n    # dataset -----------------> inputRDD --------> squareRDD\n\n    squareRDD = inputRDD.map(my_square_function)\n\n    # 3. Operation A1: 'collect'.\n\n    #         C1: parallelize             T1: map             A1: collect\n    # dataset -----------------> inputRDD --------> squareRDD ------------> resVAL\n\n    resVAL = squareRDD.collect()\n\n    # 4. We print by the screen the collection computed in resVAL\n    for item in resVAL:\n        print(item)\n\n\n# ------------------------------------------\n# FUNCTION my_power\n# ------------------------------------------\ndef my_power(a, b):\n    # 1. We create the output variable\n    res = a ** b\n\n    # 2. We return res\n    return res\n\n\ndef my_bypass(x):\n    res = my_power(x, n)\n    return res\n\n# ------------------------------------------\n# FUNCTION map_explicit_function_has_more_than_one_parameter\n# ------------------------------------------\ndef map_explicit_function_has_more_than_one_parameter(sc, n):\n    # 1. Operation C1: Creation 'parallelize', so as to store the content of the collection [1,2,3,4] into an RDD.\n\n    #         C1: parallelize\n    # dataset -----------------> inputRDD\n\n    inputRDD = sc.parallelize([1, 2, 3, 4, 5])\n\n    # 2. Operation T1: Transformation 'map', so as to get a new RDD ('squareRDD') from inputRDD.\n\n    # map requires as input arguments: (i) A function F and (ii) a collection of items C.\n    # map produces as output argument a new collection C' by applying F to each element of C.\n\n    # In our case, the collection C is always going to be RDD1, and C' the new RDD2.\n    # Thus, the only thing we are missing is specifying F.\n\n    # As we saw before, F must be a function receiving just 1 parameter (the item of C we want to apply F(C) to).\n    # This is a big limitation! What happens if we indeed want to apply a function F receiving 2 or more parameters?\n    # We can walk around it by doing the following:\n    # i) Let's call F2 the function we actually want to apply, where F2 has more than one parameter, for example 2 parameters\n    # F2(c, extra_argument) = result\n    # Now, let's define our actual F, the one map will indeed apply via the following lambda expression\n    # F --> lambda c: F2(c, extra_argument)\n    # As we see, map will still apply the function F receiving 1 parameter (happy times as F must only contain just 1 parameter).\n    # However, F will be indeed nothing but a bypass function: You call me with c, I redirect you to F2(c, extra_parameter).\n    # This way we can indeed use map to transform an RDD by applying a function with more than 1 parameter.\n\n    #         C1: parallelize             T1: map\n    # dataset -----------------> inputRDD --------> squareRDD\n\n    powerRDD = inputRDD.map(my_bypass)\n\n    # 3. Operation A1: 'collect'.\n\n    #         C1: parallelize             T1: map             A1: collect\n    # dataset -----------------> inputRDD --------> powerRDD ------------> resVAL\n\n    resVAL = powerRDD.collect()\n\n    # 4. We print by the screen the collection computed in resVAL\n    for item in resVAL:\n        print(item)\n\n\n# ------------------------------------------\n# FUNCTION my_main\n# ------------------------------------------\ndef my_main(sc, n):\n    print(\"\\n\\n--- [BLOCK 1] map with F defined via a lambda expression ---\")\n    map_lambda(sc)\n\n    print(\"\\n\\n--- [BLOCK 2] map with F defined via a explicit function ---\")\n    map_explicit_function(sc)\n\n    print(\"\\n\\n--- [BLOCK 3] map where F requires more than one parameter ---\")\n    map_explicit_function_has_more_than_one_parameter(sc, n)\n\n\n# --------------------------------------------------------\n#\n# PYTHON PROGRAM EXECUTION\n#\n# Once our computer has finished processing the PYTHON PROGRAM DEFINITION section its knowledge is set.\n# Now its time to apply this knowledge.\n#\n# When launching in a terminal the command:\n# user:~$ python3 this_file.py\n# our computer finally processes this PYTHON PROGRAM EXECUTION section, which:\n# (i) Specifies the function F to be executed.\n# (ii) Define any input parameter such this function F has to be called with.\n#\n# --------------------------------------------------------\nif __name__ == '__main__':\n    # 1. We use as many input arguments as needed\n    n = 3\n\n    # 2. Local or Databricks\n    pass\n\n    # 3. We configure the Spark Context\n    sc = pyspark.SparkContext.getOrCreate()\n    sc.setLogLevel('WARN')\n    print(\"\\n\\n\\n\")\n\n    # 4. We call to my_main\n    my_main(sc, n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2acc815e-1ff9-49f4-b1c4-57b42715e56a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\n\n\n\n\n\n--- [BLOCK 1] map with F defined via a lambda expression ---\n1\n4\n9\n16\n25\n\n\n--- [BLOCK 2] map with F defined via a explicit function ---\n1\n4\n9\n16\n25\n\n\n--- [BLOCK 3] map where F requires more than one parameter ---\n1\n8\n27\n64\n125\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\n\n\n\n\n\n--- [BLOCK 1] map with F defined via a lambda expression ---\n1\n4\n9\n16\n25\n\n\n--- [BLOCK 2] map with F defined via a explicit function ---\n1\n4\n9\n16\n25\n\n\n--- [BLOCK 3] map where F requires more than one parameter ---\n1\n8\n27\n64\n125\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"p03_map","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1468212715962377}},"nbformat":4,"nbformat_minor":0}
