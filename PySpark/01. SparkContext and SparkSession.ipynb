{"cells":[{"cell_type":"markdown","source":["##SparkContext\n\nSpark is a serious software. It takes more time to start up and running simpler computations might take longer than expected. \nThat's because all the optimizations that Spark has under its hood are designed for complicated operations with big data sets. \nThat means that for simple or small problems Spark may actually perform worse than some other solutions!"],"metadata":{}},{"cell_type":"code","source":["# Verify SparkContext\nprint(sc)\n\n# Print Spark version\nprint(sc.version)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;SparkContext master=local[8] appName=Databricks Shell&gt;\n3.0.1\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["## SparkSessions\nCreating multiple SparkSessions and SparkContexts can cause issues, so it's best practice to use the SparkSession.builder.getOrCreate() method. \nThis returns an existing SparkSession if there's already one in the environment, or creates a new one if necessary!"],"metadata":{}},{"cell_type":"code","source":["# Import SparkSession from pyspark.sql\nfrom pyspark.sql import SparkSession\n\n# Create my_spark\nmy_spark = SparkSession.builder.getOrCreate()\n\n# Print my_spark\nprint(my_spark)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;pyspark.sql.session.SparkSession object at 0x7fc57a0fec90&gt;\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["## Viewing tables\n\nAfter creating a SparkSession, it is possible to see what data exists in the cluster!\nThe SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.\n\nOne of the most useful is the .listTables() method, which returns the names of all the tables in the cluster as a list."],"metadata":{}},{"cell_type":"code","source":["# Print the tables in the catalog\nprint(spark.catalog.listTables())"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Table(name=&#39;flights&#39;, database=&#39;default&#39;, description=None, tableType=&#39;MANAGED&#39;, isTemporary=False)]\n</div>"]}}],"execution_count":6}],"metadata":{"name":"01. SparkContext and SparkSession","notebookId":1227023265356590},"nbformat":4,"nbformat_minor":0}
